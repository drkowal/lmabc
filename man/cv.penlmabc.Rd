% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv.penlmabc.R
\name{cv.penlmabc}
\alias{cv.penlmabc}
\title{Fitting Lasso/Ridge Regression with Abundance-Based Constraints (ABCs)}
\usage{
cv.penlmabc(
  formula,
  data,
  type = "lasso",
  lambda_path = NULL,
  K = 10,
  props = NULL,
  plot = FALSE
)
}
\arguments{
\item{formula}{an object of class "\code{\link[=formula]{formula()}}" (or one that can be coerced to that class); a symbolic description of the model to be fitted.}

\item{data}{an optional data frame (or object coercible by \code{as.data.frame} to a data frame) containing the variables in the model.}

\item{type}{either "lasso" or "ridge"}

\item{lambda_path}{optional vector of tuning parameters;
defaults are inherited from \code{glmnet} (for ridge) or \code{genlasso} (for lasso)}

\item{K}{number of folds for cross-validation; default is 10}

\item{props}{an optional named list with an entry for each named categorical variable in the model, specifying the proportions of each category. By default, \code{props} will be calculated from the empirical proportions in the data.}

\item{plot}{logical; if TRUE, include a plot of the cross-validated
MSE across \code{lambda_path} values}
}
\description{
\code{cv.penlmabc} fits penalized (lasso or ridge) linear models
using abundance-based constraints (ABCs). For penalized regression
with categorical covariates, ABCs eliminate harmful biases
(e.g., with respect to race, sex, religion, etc.), provide
more meaningful notions of sparsity, and improve interpretability.
}
\section{Details}{
\code{cv.penlmabc} solves the penalized least squares problem

\eqn{|| y - X\beta||^2 + \lambda \sum_j |\beta_j|^\gamma}

for lasso (\eqn{\gamma=1}) or ridge (\eqn{\gamma=2}) regression,
and specifically using ABCs for categorical covariates and interactions.

Default strategies for categorical covariates typically
use reference group encoding (RGE), which removes
the coefficient for one group for each
categorical variable (and their interactions).
However, because lasso and ridge shrink coefficients
toward zero, this implies that all other coefficients are
\emph{biased} toward their reference group. Such bias is
clearly problematic for variables such as race, sex, and other
protected groups, but also it attenuates the estimated differences
among groups, which can obscure important group-specific
effects (e.g., for \code{x:race}). The penalized estimates and predictions
under RGE are dependent on the choice of the reference group.

Alternatively, it is possible to fit penalized least squares
with an overparametrized model, i.e., without deleting a
reference group (or applying any types of constraints). However,
the parameters are not identifiable and thus not interpretable
in general. With lasso estimation, this approach empirically tends
to select a reference group, and therefore suffers
from the same significant problems as RGE.

Instead, ABCs provide a parametrization of the main
effects as "group-averaged" effects, with interaction terms
(e.g., \code{x:race}) as "group-specific deviations".
These estimators are not biased toward any single group
and the predictive performance does not depend on the
choice of a reference group. ABCs provide appealing estimation
invariance properties for models with or without interactions
(e.g., \code{x:race}), and therefore offer a natural parametrization
for sparse (e.g., lasso) estimation with categorical covariates
and their interactions.
}

\section{Value}{
\code{cv.penlmabc} returns a list with the following elements:
\itemize{
\item \code{coefficients} estimated coefficients at each tuning
parameter in \code{lambda_path}
\item \code{lambda_path} vector of tuning parameters
\item \code{df} degrees of freedom at each tuning
parameter in \code{lambda_path}
\item \code{mse} cross-validated mean squared error (MSE) at each tuning
parameter in \code{lambda_path}
\item \code{se} standard error of the CV-MSE at each tuning
parameter in \code{lambda_path}
\item \code{ind.min} index of the minimum CV-MSE in \code{lambda_path}
\item \code{ind.1se} index of the one-standard error rule in \code{lambda_path}
\item \code{lambda.min} tuning parameter that achieves the minimum CV-MSE
\item \code{lambda.1se} tuning parameter that achieves the one-standard error rule
}
}

\examples{
# Example lasso fit:
fit <- cv.penlmabc(Sepal.Length ~ Petal.Length + Species + Petal.Length*Species, data = iris)
names(fit)

# Estimated coefficients at the one-standard error rule:
coef(fit)[,fit$ind.1se]

}
